{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ee1e12",
   "metadata": {},
   "source": [
    "* 回归-（输入变量和输出变量是连续）\n",
    "* 分类 -(输入变量是连续的但是输出是有限的)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c68910",
   "metadata": {},
   "source": [
    "* 线性回归\n",
    "  * 线性回归\n",
    "    * 普通回归LinearRegression(矩阵求解，没有正则化项)\n",
    "      * 一元线性回归 y=kx+b\n",
    "      * 一元多项式回归 $y=\\beta_0 ++\\beta_1 * x+ \\beta_2 * x^2+ \\beta_3 * x^3+ \\beta_4 * x^4+ \\beta_5 * x^5 +b$\n",
    "      * 多元线性回归 $y=\\beta_0 +\\beta_1 * x_1 + \\beta_2 * x_2 + \\beta_3 * x_3+b$\n",
    "      \n",
    "    * 岭回归 Ridge\n",
    "    * LASSO回归\n",
    "    * 贝叶斯回归\n",
    "  * 逻辑回归\n",
    "\n",
    "* 数据处理 \n",
    "  ```\n",
    "  归一化/标准化 都是为了处理特征值\n",
    "    * 归一化：将特征值转换到[0,1]区间\n",
    "    * 标准化：将特征值转换到[-1,1]区间\n",
    "  1:加快梯度下降速度\n",
    "  2:有可能提高精度\n",
    "  3:减少某些特征值异常过大/小数据对预测的影响\n",
    "  ```\n",
    "  * 归一化\n",
    "    $ X_{norm}=\\frac{X-X_{min}}{X_{max}-X_{min}} $\n",
    "    * 归一化方法比较适用在数值比较集中\n",
    "    * 归一化 max,min 影响很大。异常大/小的数据 会造成效果不理想。可以使用经验值代替max和min\n",
    "\n",
    "  * 标准化\n",
    "    $ X_{norm} = \\frac{x-x_{mean}}{σ} $\n",
    "    * 标准化的意义在于解决归一化的缺陷\n",
    "    * 具有一定数据量，少量的异常点对平均值的影响不是那么大\n",
    "  \n",
    "  * 缺失值（一条数据缺少某些特征值）\n",
    "    * 可用该列特征值的平均数或中位数进行填补 sklearn.impute.SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9775277",
   "metadata": {},
   "source": [
    "* 一维线性函数预测 \n",
    "    $\n",
    "      y = wx + b\n",
    "    $\n",
    "    * 损失函数\n",
    "      * 把图中预测值和真实值的差值求和，然后求平均值，就是损失函数\n",
    "      $\n",
    "        L(y,y_i) = \\sum_{i=1}^n (y - y_i)^2\n",
    "      $\n",
    "      * 均值误差 (其他:欧式距离,余弦距离,曼哈顿距离，马氏距离，总偏差平方和,回归平方和，残差平方和)\n",
    "        $\n",
    "          L(w,b) = \\frac{1}{n} \\sum_{i=1}^n (wx_i + b - y_i)^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i- wx_i - b )^2\n",
    "        $\n",
    "          * 为什么会选择误差平方（最小二乘法） 来标识显示回归问题的损失函数 [](https://www.zhuanlan.zhihu.com/p/147297924)\n",
    "          \n",
    "      * 线性回归问题就是求损失函数的最小值时的(w,b)来确定预测函数,进而预测其他值\n",
    "        * 对b 偏导数\n",
    "          $\n",
    "            \\frac{\\partial f}{\\partial b} = -2(\\sum_{i=1}^n y_i -nb - w\\sum_{i=1}^n x_i)\n",
    "          $\n",
    "        * 对w 偏导数\n",
    "\n",
    "          $\n",
    "            \\frac{\\partial f}{\\partial w} = -2(\\sum_{i=1}^n y_i * x_i -\\sum_{i=0}^n x_i - w\\sum_{i=1}^n x_i)\n",
    "          $\n",
    "        * 令 $ \\frac{\\partial f}{\\partial w}=0$ 以及 $ \\frac{\\partial f}{\\partial b}=0$\n",
    "  \n",
    "          $\n",
    "            w = \\frac{n\\varSigma y_i * x_i -\\varSigma x_i*\\varSigma y_i}{n\\varSigma x_i^2 -(\\varSigma x_i)^2}\n",
    "          $\n",
    "\n",
    "          $\n",
    "            b = \\frac{\\varSigma x_i^2 *\\varSigma y_i -\\varSigma x_i*\\varSigma y_i x_i}{n\\varSigma x_i^2 -(\\varSigma x_i)^2}\n",
    "          $\n",
    "          ==>\n",
    "          $\n",
    "            w = \\frac{\\overline{X} * \\overline{Y} - \\overline{(XY)}}{\\overline{X}^2  - \\overline{X^2}}\n",
    "          $\n",
    "\n",
    "          $\n",
    "            b = \\overline{Y} - w\\overline{X}\n",
    "          $\n",
    "    <img src='./images/image01.png'> <img width='330px' src='./images/image02.png'>\n",
    "    <br/>\n",
    "    <br/>\n",
    "    <br/>\n",
    "    * 一维回归--多项回归（是否使用可以看数据分布情况）\n",
    "      * 一维数据使用多项式模型进行回归预测\n",
    "        * 例如：一元二项回归 U行曲线回归预测 $y = \\beta_0 + \\beta_1 * x + \\beta_2 * x^2 + b $\n",
    "      * sklearn.preprocessing.PolynomialFeatures 通过一维数据构建多项式数据\n",
    "      $\n",
    "      y=\\beta_0 ++\\beta_1 * x+ \\beta_2 * x^2+ \\beta_3 * x^3+ \\beta_4 * x^4+ \\beta_5 * x^5 +b\n",
    "      $\n",
    "      * 相对于一维回归问题，多项式回归问题 解决数据分布不适合用一维回归(kx+b)预测的问题\n",
    "      <br/>\n",
    "      <br/>\n",
    "      <br/>\n",
    "      <img width='330px' src='./images/image03.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4261c01",
   "metadata": {},
   "source": [
    "* 逻辑回归（逻辑分类）\n",
    "\n",
    "    * 逻辑回归是回归函数+阈值来处理分类问题\n",
    "    * 逻辑回归 = 线性回归 + Sigmoid函数 (把线性回归函数的值在通过Sigmoid来映射到(10,1),再通过阈值来进行分类)\n",
    "      * 线性回归\n",
    "          $$\n",
    "           Z= W*x +b\n",
    "          $$\n",
    "      * Sigmoid函数 （是一类类似S形状的函数，该讲解的Sigmoid函数指的是下面标准函数）\n",
    "           $$\n",
    "           Y=\\frac{1}{(1+{\\rm e}^{-z})}\n",
    "           $$\n",
    "      * 逻辑回归\n",
    "          $$\n",
    "            Y=\\frac{1}{(1+{\\rm e}^{-(W*x + b)})}  \n",
    "          $$\n",
    "          \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81652d4",
   "metadata": {},
   "source": [
    "# SKlearn\n",
    "* 线性回归\n",
    "  * 普通回归LinearRegression(矩阵求解，没有正则化项)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
